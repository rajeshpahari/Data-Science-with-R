#Accuracy of the model
accuracy<-table(p1,testData[,"label"])
sum(diag(accuracy))/sum(accuracy)
#Overall accuracy is 96%
varImp(mod_fit2)
#No exact equivalent of R-Sqr is avilable like LM
#the Mcfadden R-Sqr index can be used to asses the model fit
#to do that we have to create different model
mod_fit2a<-glm(label~Q25+Q75+sp.ent+sfm+meanfun+minfun+mode,data=trainData,family="binomial")
#install.packages("pscl")
library(pscl)
pR2(mod_fit2a)
#87% variance in response variable
require(caret)
tumor<-read.csv("C:\\Personal\\Learning\\Certification\\R and ML\\Course-Script-1\\Course_Script_1\\Sentimental-Analysis_R-master\\Sentimental-Analysis_R-master\\cancer_tumor.csv")
#Prepare working data....drop unnecessary data
drops=c("id","X")
df<-tumor[,!(names(tumor)%in%drops)]
table(df$diagnosis)
prop.table(table(df$diagnosis))
corr_mat<-cor(df[,2:ncol(df)])#predictors starts from col -2
library(corrplot)
corrplot(corr_mat)
#There are strong co-relations for few variables
pcav<-prcomp(df[,2:ncol(df)],center = TRUE,scale=TRUE)
# we do centering & Scaling as well
plot(pcav,type="l")
pcf_df<-as.data.frame(pcav$x)
pcf_df
library(ggplot2)
ggplot(pcf_df,aes(x=PC1,y=PC2,col=df$diagnosis))+geom_point()
set.seed(99)
#Data Prep
id<-createDataPartition(df$diagnosis,p=0.75,list=FALSE)
head(id)
trainData=voice[id,]#Training Data
testData=voice[-id,]#Testing Data
head(trainData)
#Define training control
fitControl=trainControl(method="cv",number=10,   #10 fold cross validation
preProcOptions = list(thresh=.95),
classProbs = TRUE,
summaryFunction = twoClassSummary)
#LDA as classifier
fit_1<-train(diagnosis~.,
trainData,
method='lda',
metric='ROC',
preProcess=c("center","scale","pca"),
tuneLength=10,
trace=FALSE,
trControl=fitControl)
names(trainData)
names(df)
prop.table(table(df$diagnosis))
head(id)
trainData=voice[id,]#Training Data
testData=voice[-id,]#Testing Data
head(trainData)
name(trainData)
names(trainData)
training=voice[id,]#Training Data
testing=voice[-id,]#Testing Data
names(training)
id<-createDataPartition(df$diagnosis,p=0.75,list=FALSE)
head(id)
training=df[id,]#Training Data
testing=df[-id,]#Testing Data
names(training)
#Define training control
fitControl=trainControl(method="cv",number=10,   #10 fold cross validation
preProcOptions = list(thresh=.95),
classProbs = TRUE,
summaryFunction = twoClassSummary)
#LDA as classifier
fit_1<-train(diagnosis~.,
training,
method='lda',
metric='ROC',
preProcess=c("center","scale","pca"),
tuneLength=10,
trace=FALSE,
trControl=fitControl)
pred_1=predict(fit_1,newdata = testing)
cm_lda=confusionMatrix(pred_1,testing$diagnosis,Positive="M")
cm_lda=confusionMatrix(pred_1,testing$diagnosis,positive="M")
cm_lda
#nayve Byase Model
fit_1<-train(diagnosis~.,
training,
method='nb',
metric='ROC',
preProcess=c("center","scale","pca"),
tuneLength=10,
trace=FALSE,
trControl=fitControl)
pred_1=predict(fit_1,newdata = testing)
cm_lda=confusionMatrix(pred_1,testing$diagnosis,positive="M")
cm_lda
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(mlbench)
data("Sonar")
head(sonar)
head(Sonar)
#Training and test data prep....60% & 40%
tr<-sample(nrow(Sonar),round(nrow(Sonar)*.6))
trainData<-Sonar[tr,]
testData<-Sonar[-tr,]
data("Sonar")
head(Sonar)
unique(Sonar$Class)
model<-glm(Class~.,data=trainData,family = "binomial")
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(mlbench)
data("Sonar")
head(Sonar)
#unique(Sonar$Class)
#Training and test data prep....60% & 40%
tr<-sample(nrow(Sonar),round(nrow(Sonar)*.6))
trainData<-Sonar[tr,]
testData<-Sonar[-tr,]
model<-glm(Class~.,data=trainData,family = "binomial")
library(caret)
library(mlbench)
p<- predict(model,test,type=response)
p<- predict(model,test,type="response")
p<- predict(model,testData,type="response")
summary(p)
cl<-ifelse(p>0.5,"M","R")
confusionMatrix(cl,testData$Class)
table(cl,testData$Class)
confusionMatrix(cl,testData$Class)
confusionMatrix(cl,testData$Class)
knitr::opts_chunk$set(echo = TRUE)
2+3
plot(pressure)
names(knitr::knit_engines$get())
plot(pressure)
names(knitr::knit_engines$get())
knitr::opts_chunk$set(engine.path = list(
python = '~/anaconda/bin/python',
))
plot(pressure)
names(knitr::knit_engines$get())
# Install reticulate package
install.packages(“reticulate”)
2+3
install.packages(“reticulate”)
2+3
install.packages("reticulate")
# Load reticulate package
library(reticulate)
py_available()
# Load reticulate package
library(reticulate)
py_available()
2+3
install.packages("reticulate")
# Load reticulate package
library(reticulate)
py_available()
py_config()
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
scipy <- import("scipy")
scipy$amin(c(1,3,5,7))
library(reticulate)
scipy <- import("scipy")
scipy$amin(c(1,3,5,7))
library(reticulate)
knitr::knit_engines$set(python = reticulate::eng_python)
x = 'hello, python world!'
x
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
print("Mean:" + mean(data))
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
print("Mean:" + mean(data))
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
print("Mean: %i" + mean(data))
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
print("Mean: %i", mean(data))
#Median
print("Median:" + median(data))
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
sprintf("Mean: %i", mean(data))
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
mean(data)
#Median
print("Median:" + median(data))
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
mean(data)
#Median
median(data)
#Mode
y=table(data)
mode<-names(y)which[y==max(y)]
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
mean(data)
#Median
median(data)
#Mode
mode(data)
y=table(data)
mode<-names(y)which[y==max(y)]
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
mean(data)
#Median
median(data)
#Mode
y=table(data)
y
mode<-names(y)which[y==max(y)]
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
mean(data)
#Median
median(data)
#Mode
y=table(data)
y
mode<-names(y)[which(y==max(y))]
data= c(2,3,4,5,6,9,8,7,2,3,4,3)
#Mean
mean(data)
#Median
median(data)
#Mode
y=table(data)
y
mode<-names(y)[which(y==max(y))]
mode
data
data
var(data)
sd(data)
data
var(data)
sd(data)
data
var(data)
sd(data)
range(data)
data
var(data)
sd(data)
range(data)
IQR(data)
data(iris)
summary(iris)
#distribution in terms of sepal length and speal width
library(ggplot2)
ggplot(iris,aes(Sepal.Length,Sepal.Width))+geom_point(aes(colour=Species))
###########k-means clustgering
set.seed(200)
desired_cluster<-3
irishcluster<-kmeans(iris[,1:2],desired_cluster,nstart = 15)
irishcluster
t<-table(irishcluster$cluster,iris$Species)
t
irishcluster$cluster<- as.factor(irishcluster$cluster)
ggplot(iris,aes(Sepal.Length,Sepal.Width))+geom_point(aes(colour=irishcluster$cluster))
###################How to find optimum clustering
max<-15
data<-scale(iris[,-5])
wss<-sapply(1:max, function(k){kmeans(data,k,nstart = 50,iter.max = 15)$tot.withinss})
plot(1:max,wss,type="b",pch=19,frame=F,
xlab="Number of cluster",
ylab="Total within-Cluster sum of squares")
library(factoextra)
fviz_cluster(irishcluster,data=data,geom="point",
stand=F,frame.type="norm")
##################################################
data<-iris[,-5]
head(data)
library(NbClust)
nb=NbClust(data,method="kmeans")
nb$Best.nc
hist(nb$Best.nc[1,],breaks=15)
##################Fuzzy k-means clusering
#one pint can be in multiple cluster
require(e1071)
head(USArrests)
df<-scale(USArrests)
desired_cluster<-6
iter_max<-20
cl<-cmeans(df,desired_cluster,iter_max,verbose = F,method="cmeans")
head(cl)
?fclustIndex()
fclustIndex(cl,df,index="all")
require(fclust)
data("unemployment")
head(unemployment)
?FKM()
desired_cluster=3
unempFKM<-FKM(df,desired_cluster,stand=1)
unempFKM
summary(unempFKM)
plot(unempFKM,v1v2=c(1,3))
plot(unempFKM,v1v2=c(1,3),umean=.2)
?v1v2
g=FKM.gk(unemployment)
summary(g)
plot(g)
VIFCR(g,which)
require(cluster)
df<-scale(USArrests)
desired_cluster<-3
resf<-fanny(df,desired_cluster)
head(resf$membership,3)
head(resf$clustering)
library(factoextra)
?fviz_cluster()
fviz_cluster(resf,ellipse.type = "norm",repel=T,palette="jco",
ggtheme = theme_minimal(),legend="right")
#Goodness of clustering result
fviz_silhouette(resf,palette="jco",gg_theme=theme_minimal())
df2<-Scale(unemployment)
resf2<-fanny(df,3)
head(resf2$membership,3)
df2<-Scale(unemployment)
resf2<-fanny(df,3)
head(resf2$membership,3)
resf2<-fanny(df2,3)
head(resf2$membership,3)
df2<-Scale(unemployment)
resf2<-fanny(df2,3)
head(resf2$membership,3)
df2<-scale(unemployment)
resf2<-fanny(df2,3)
head(resf2$membership,3)
head(resf2$clustering)
fviz_cluster(resf2,ellipse.type = "norm",repel = T,palette="jco",
ggtheme = theme_minimal(),legend="right")
fviz_silhouette(resf2,palette="jco",gg_theme=theme_minimal())
plot(fanny(USArrests,6))
require(fclust)
data("unemployment")
head(unemployment)
x<-scale(unemployment)
#weigthed-k-means
?ewkm
#weigthed-k-means
??ewkm
mywkm<-ewkm(x,3,lambda=1,maxiter = 100)
library(wskm)
require(fclust)
mywkm<-ewkm(x,3,lambda=1,maxiter = 100)
#weigthed-k-means
??ewkm
myewkm<-ewkm(x,3,lambda=1,maxiter = 100)
head(myewkm$cluster)
head(myewkm$centers)
head(myewkm$totss)
head(myewkm$withinss)
head(myewkm$iterations)
head(myewkm$restarts)
plot(x,myewkm$cluster)
plot(x,col=myewkm$cluster)
plot(myewkm)
#Hierarchical clustering
library(fclust)
library(cluster)
df<-scale(unemployment)
head(df)
#dissimilarity matrix
d<-dist(df,method = "euclidean")
head(d)
#hierarchical clustering using compete linkage
hcl<-hclust(d,method="complete")
plot(hcl,cex=.6,hang=-1)
plot(hcl,cex=1.6,hang=-1)
plot(hcl,cex=.6,hang=-1)
plot(hcl,cex=.6,hang=-10)
plot(hcl,cex=.6,hang=-10)
plot(hcl,cex=.6,hang=-100)
plot(hcl,cex=.6,hang=-1)
#compute with agnes & complete linkage
hc2<-agnes(df,method = "complete")
plot(hc2,cex=.6,hang=-1)
hc2$ac
pltree(hc2,cex=.6,hang=-1)
#agend & method wards
hc3<-agens(df,method="ward")
pltree(hc3,cex=.6,hang=-1)
#####Devisive HC-Diana Method
hc4<-diana(df)
hc4$dc
pltree(hc4,cex=.6,hang=-1)
#wards method
hc5<-hclust(d,method="ward.D2")
#create sub-groups
sub_grps<-cutree(hc5,k=4)
table(sub_grps)
library(tidyverse)
unemployment%>%mutate(cluster=sub_grps)%>%head
??mutate
plot(hc5,cex=.6,)
rect.hclust(hc5,k=4,border=2:5)
rect.hclust(hc5,k=5,border=2:5)
rect.hclust(hc5,k=5,border=20:5)
rect.hclust(hc5,k=5,border=4:5)
rect.hclust(hc5,k=5,border=1:5)
rect.hclust(hc5,k=5,border=2:5)
rect.hclust(hc5,k=4,border=2:5)
rect.hclust(hc5,k=5,border=2:6)
library(factoextra)
fviz_cluster(list(data=df,cluster=sub_grps))
#But what is optimal cluster?
fviz_nbclust(df,FUN=hcut,method="silhouette")
fviz_nbclust(df,FUN=hcut,method="wss")
rect.hclust(hc5,k=2,border=2:6)
plot(hc5,cex=.6,)
rect.hclust(hc5,k=2,border=2:6)
##EM Algo- Expectation Maximization
library(mclust)
library(fclust)
x<-scale(unemployment)
mc<-Mclust(x)
summary(mc)
mc$call
mc$modelName
mc$n
mc$d
mc$G
mc$BIC
mc$z
head(mc$classification)
head(mc$uncertainty)
head(mc$BIC)
libary(factoextra)
library(factoextra)
fviz_mclust(mc,"BIC",palette="jco")
fviz_mclust(mc,"classification",geom="point",palette="jco",pointsize=1.5)
fviz_mclust(mc,"uncertainty",palette="jco")
##Dbscan clustering in R
library(dbscan)
data(iris)
head(iris)
x<-as.matrix(iris[,1:4])
head(x)
#dbscan method
?dbscan()
db<-dbscan(x,eps=.4,minPts = 4)
db
#optimal eps
dbscan::kNNdistplot(x,k=5)
dbscan::kNNdistplot(x,k=5)
abline(h=.15,lty=2)
res.fpc<-fpc::dbscan(x,eps=.4,minPts=4)
db<-dbscan(x,eps=.4,MinPts = 4)
db
#optimal eps
dbscan::kNNdistplot(x,k=5)
dbscan::kNNdistplot(x,k=5)
abline(h=.15,lty=2)
res.fpc<-fpc::dbscan(x,eps=.4,MinPts=4)
res.db<-dbscan(x,.4,4)
require(fpc)
require(factoextra)
fviz_cluster(res.fpc,x,geom="point")
#Rattle Unsupervised classification
require(rattle)
rattle()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
data(iris)
head(iris)
#skewness can eefc the result that why data pre-processing is required.
#lets do log trnsformation
log.x<-log(iris[,1:4])
ir.species<-iris[,5]
ir.pca<-(log.x,scale=T,center=T)
ir.pca<-prcomp(log.x,scale=T,center=T)
pinrt(pr.pca)
pinrt(ir.pca)
print(ir.pca)
plot(ir.pca,type="l")
summary(ir.pca)
###vsulaise data more intutive way
library(devtools)
install_github('fawda123/ggord')
library(ggord)
p<-ggord(ir.pca,iris$Species)
p
library(FactoMineR)
library(factoextra)
res.pca<-PCA(log.x,graph=F)
get_eig(res.pca)
#visualize
fviz_screeplot(res.pca,addlabels=T,ylim=c(0,90))
#How much individual vars contributed to PC variance
var<-get_pca_var(res.pca)
var
head(var$contrib)
fviz_contrib(res.pca,choice="var",axes=1,top=10)#PC1
fviz_contrib(res.pca,choice="var",axes=2,top=10)#PC1
fviz_contrib(res.pca,choice="var",axes=1,top=10)#PC1
fviz_pca_var(res.pca,color="black")
fviz_pca_var(res.pca,col.var="black")
fviz_pca_var(res.pca,col.var="red")
fviz_pca_var(res.pca,col.var="orange")
